Peform a machine learning experiment to investigate whether pretrained GPT-2 (124M parameters) can improve its performance in integer multiplication through fine-tuning.

TASK:
1. Evaluate baseline GPT-2 on multiplication problems of varying difficulty (single-digit through multi-digit)
2. Design and implement a fine-tuning approach to improve performance
3. Re-evaluate and measure improvements
4. Write a research paper (4-6 pages) with your findings

TECHNICAL NOTES:
- GPT-2 (124M) is already downloaded and available via transformers library
- MANDATORY: Use Modal for ALL GPU operations (evaluation, training, inference)
  * Import modal and create app: app = modal.App("gpt2-multiplication")
  * Decorate compute functions with: @app.function(gpu="T4", timeout=1800)
  * Modal is already installed and authenticated
  * This is REQUIRED every iteration - do not run locally
- Keep total compute time to 10-30 minutes on GPU (budget <$1)
- Generate training/test data programmatically

PAPER REQUIREMENTS:
- Standard sections: Introduction, Methods, Results, Discussion, Conclusion
- Include tables/figures comparing baseline vs fine-tuned performance
- Write in complete paragraphs (no bullet lists in main text)
- Anonymous authorship
- Write as a human researcher would

See nanoGPT codebase context below for reference on model loading and training.
