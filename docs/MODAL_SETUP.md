# Modal Setup Guide

This guide walks you through setting up Modal for GPU-accelerated ML experiments.

## What is Modal?

Modal is a serverless compute platform that lets you run Python functions on powerful GPUs without managing infrastructure. Perfect for:
- Fine-tuning language models
- Running training experiments
- GPU-intensive computations
- All from your local Python code!

## Pricing

**Free Tier:**
- $30 free credits per month
- No credit card required to start
- Perfect for research experiments

**GPU Costs:**
- T4 GPU: ~$0.60/hour (recommended for gpt2/gpt2-medium)
- A10G GPU: ~$1.20/hour (for larger models)
- A100 GPU: ~$3.00/hour (for production workloads)

**For your research:**
- Fine-tuning gpt2: ~$0.05-0.10 per experiment (5-10 minutes)
- Fine-tuning gpt2-medium: ~$0.10-0.30 per experiment (10-30 minutes)
- 10 experiments: ~$1-3 total
- Free tier covers ~10-30 experiments per month!

## Setup Steps

### 1. Create a Modal Account

1. Go to: **https://modal.com/**
2. Click **"Sign Up"** (top right)
3. Choose your sign-up method:
   - **GitHub** (recommended - fastest)
   - Google
   - Email + password
4. Complete the registration

### 2. Verify Modal CLI is Installed

Modal is already installed in your environment. Verify with:

```bash
modal --version
```

If needed, install with:
```bash
pip install modal
```

### 3. Authenticate Modal CLI

Run this command in your terminal:

```bash
modal token new
```

**What happens:**
1. A browser window opens automatically
2. You're asked to log in to Modal (if not already logged in)
3. You'll see an authorization page
4. Click **"Authorize"** or **"Create Token"**
5. The token is automatically saved to `~/.modal.toml`
6. You see a success message in the terminal

**If the browser doesn't open:**
1. The command will print a URL like: `https://modal.com/token-flow/...`
2. Copy and paste that URL into your browser
3. Complete the authorization
4. The token saves automatically

**Token storage:**
- Location: `~/.modal.toml` (in your home directory)
- Safe from git commits (outside repository)
- Shared across all your projects

### 4. Test Your Setup

After authentication, test that everything works:

```bash
python test_modal_setup.py
```

This script will:
1. ✓ Test basic Modal functionality
2. ✓ Verify GPU access (T4 GPU)
3. ✓ Run a simple computation on GPU

**Expected output:**
```
======================================================================
  MODAL SETUP VERIFICATION
======================================================================

[1/3] Testing basic Modal functionality...
✓ Hello from Modal! Python version: 3.12.x

[2/3] Testing GPU access (this may take a moment to provision)...
✓ GPU test passed! Using Tesla T4
  Device: Tesla T4
  GPU count: 1

[3/3] Testing GPU computation...
✓ Computation test passed on cuda
  Matrix shape: [1000, 1000]

======================================================================
  ALL TESTS PASSED!
======================================================================
```

**Note:** The first run may take 30-60 seconds as Modal provisions a GPU container.

## Using Modal in Your Research

### Example 1: Simple GPU Function

```python
import modal

app = modal.App("my-research")

@app.function(
    gpu="T4",
    image=modal.Image.debian_slim().pip_install("torch", "transformers"),
)
def finetune_model(dataset_path, model_name="gpt2"):
    """Fine-tune a model on Modal GPU."""
    import torch
    from transformers import GPT2LMHeadModel, Trainer, TrainingArguments

    # Your training code here
    model = GPT2LMHeadModel.from_pretrained(model_name)

    # ... training logic ...

    return {"status": "success", "loss": final_loss}

# Run locally, executes on Modal GPU
if __name__ == "__main__":
    result = finetune_model.remote("data/my_dataset.txt")
    print(result)
```

### Example 2: With nanoGPT

```python
import modal

app = modal.App("nanogpt-training")

# Define the container image with all dependencies
image = (
    modal.Image.debian_slim()
    .pip_install("torch", "transformers", "tiktoken", "numpy")
)

@app.function(
    gpu="T4",
    image=image,
    timeout=3600,  # 1 hour
)
def train_nanogpt(config):
    """Train or fine-tune using nanoGPT code."""
    # Your nanoGPT training code here
    # This runs on Modal's GPU infrastructure
    pass

# Run from local machine
if __name__ == "__main__":
    with app.run():
        result = train_nanogpt.remote(my_config)
```

### Example 3: Research Agent Integration

The research agent can automatically generate Modal code:

```python
# In your experiment code (generated by the agent)
import modal

app = modal.App("research-experiment")

@app.function(gpu="T4", timeout=1800)
def run_experiment():
    # Agent-generated code runs on GPU
    from transformers import GPT2LMHeadModel

    model = GPT2LMHeadModel.from_pretrained("gpt2")
    # ... experiment code ...

    return results

# Agent can call this to run on GPU
results = run_experiment.remote()
```

## Monitoring Usage and Costs

### View Your Credits

1. Go to: **https://modal.com/settings/usage**
2. See:
   - Total credits used
   - Credits remaining
   - Breakdown by function/GPU type
   - Historical usage

### Cost Control Tips

1. **Set timeouts:**
   ```python
   @app.function(gpu="T4", timeout=600)  # 10 minute max
   ```

2. **Start with T4 GPUs:**
   - Cheapest option (~$0.60/hour)
   - Perfect for gpt2 and gpt2-medium
   - Upgrade only if needed

3. **Use the free tier:**
   - $30/month = ~50 hours of T4 GPU time
   - More than enough for research experiments

4. **Monitor your usage:**
   - Check dashboard regularly
   - Set up alerts if needed

## Troubleshooting

### "No token found" Error

```bash
# Re-run token setup
modal token new
```

### "GPU request denied" Error

- Check your Modal credits at https://modal.com/settings/usage
- Verify your account is activated
- Try a smaller GPU (T4 instead of A100)

### Function Times Out

```python
# Increase timeout (in seconds)
@app.function(gpu="T4", timeout=3600)  # 1 hour
```

### Out of Memory on GPU

```python
# Use smaller batch size
# Or use a larger GPU
@app.function(gpu="A10G")  # More VRAM than T4
```

### Import Errors

```python
# Add missing packages to image
image = modal.Image.debian_slim().pip_install(
    "torch",
    "transformers",
    "tiktoken",
    "numpy",
    # ... add any other packages you need
)

@app.function(image=image, gpu="T4")
def my_function():
    pass
```

## Advanced: Secrets and Environment Variables

If you need API keys or secrets in your Modal functions:

```python
# Create secret at: https://modal.com/secrets
# Then reference it:
@app.function(
    gpu="T4",
    secrets=[modal.Secret.from_name("my-api-key")]
)
def function_with_secrets():
    import os
    api_key = os.environ["API_KEY"]
    # Use the secret
```

## Next Steps

1. ✓ Complete Modal account setup
2. ✓ Run `modal token new` to authenticate
3. ✓ Run `python test_modal_setup.py` to verify
4. ✓ Start running GPU experiments!

Your research agent can now use Modal to run GPU-accelerated experiments on nanoGPT!

## Resources

- Modal Documentation: https://modal.com/docs
- Modal Examples: https://modal.com/docs/examples
- Usage Dashboard: https://modal.com/settings/usage
- Support: https://modal.com/support

## Quick Reference

```bash
# Setup
modal token new                    # Authenticate
modal token check                  # Verify token

# Running code
python your_modal_script.py        # Run locally, executes on Modal
modal run your_modal_script.py     # Alternative syntax

# Deploy (for production)
modal deploy your_modal_script.py  # Deploy as web endpoint

# Monitoring
modal app list                     # List your apps
# Check usage at: https://modal.com/settings/usage
```
